{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Noisy-Augmented Classifier Performance\n",
        "\n",
        "This notebook trains a classifier on both real and noisy-augmented data across multiple subset sizes, evaluating on several validation sets and multiple random seeds. The results (mean ± std) serve as a reference for comparing noise-augmented models against the baseline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Setup\n",
        "\n",
        "Load all necessary libraries, dataset helpers, and set up the device and loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "\n",
        "from pkldataset import PKLDataset, NoisyPKLDataset\n",
        "from helpers import set_seed, get_model, train_model, eval_model\n",
        "\n",
        "# Device and loss criterion\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "- **Train paths**: directories containing real data pickle files for various subset sizes  \n",
        "- **Validation paths**: held-out datasets for evaluation  \n",
        "- **Seeds**: for estimating training stability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training subsets (real data)\n",
        "train_paths = [\n",
        "    \"../datasets/RPDC197/train_20\",\n",
        "    \"../datasets/RPDC197/train_50\",\n",
        "    \"../datasets/RPDC197/train_100\",\n",
        "    \"../datasets/RPDC197/train_200\",\n",
        "    \"../datasets/RPDC197/train_300\",\n",
        "    \"../datasets/RPDC197/train_400\",\n",
        "    \"../datasets/RPDC197/train_500\",\n",
        "    \"../datasets/RPDC197/train_600\",\n",
        "]\n",
        "\n",
        "# Validation sets\n",
        "val_paths = [\n",
        "    \"../datasets/RPDC185/val_1000\",\n",
        "    \"../datasets/RPDC188/val_1000\",\n",
        "    \"../datasets/RPDC191/val_1000\",\n",
        "    \"../datasets/RPDC194/val_1000\",\n",
        "    \"../datasets/RPDC197/val_1000\",\n",
        "]\n",
        "\n",
        "# Random seeds for reproducibility\n",
        "seeds = [101, 202, 303, 404, 505, 606, 707, 808, 909, 1001]\n",
        "\n",
        "# Results container: {train_path: {val_path: [accuracies]}}\n",
        "results = {tp: {vp: [] for vp in val_paths} for tp in train_paths}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training with Real + Noisy Data & Evaluation Loop\n",
        "\n",
        "For each seed:\n",
        "1. Set the random seed  \n",
        "2. For each training subset:\n",
        "   - Load real and noisy datasets, concatenate. The training dataset is augmented by combining the original data (`PKLDataset`)\n",
        "with a noise-injected version (`NoisyPKLDataset`). These two datasets are merged using\n",
        "ConcatDataset, effectively doubling the training data with added variability introduced by\n",
        "jitter. Additionally, magnitude scaling is applied alongside jittering to enhance diversity.\n",
        "For parameter details, refer to the `NoisyPKLDataset` class in `pkldataset.py`.\n",
        "   - Instantiate model, optimizer, scheduler  \n",
        "   - Train for 100 epochs  \n",
        "3. Evaluate on every validation set and record accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    for seed in seeds:\n",
        "        print(f\"\\n=== Seed {seed} ===\")\n",
        "        set_seed(seed)\n",
        "\n",
        "        for tp in train_paths:\n",
        "            print(f\"-- Training on {tp} (real + noisy)\")\n",
        "            ds_real = PKLDataset(tp)\n",
        "            ds_noisy = NoisyPKLDataset(tp)\n",
        "            combined = ConcatDataset([ds_real, ds_noisy])\n",
        "            train_loader = DataLoader(combined, batch_size=32, shuffle=True)\n",
        "\n",
        "            model = get_model().to(device)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "\n",
        "            model = train_model(\n",
        "                model,\n",
        "                train_loader,\n",
        "                criterion,\n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                num_epochs=100,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            for vp in val_paths:\n",
        "                val_loader = DataLoader(PKLDataset(vp), batch_size=64, shuffle=False)\n",
        "                acc = eval_model(model, val_loader, device)\n",
        "                results[tp][vp].append(acc)\n",
        "                print(f\"[{tp} -> {vp}] Seed {seed}: Acc = {acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Summary of Results\n",
        "\n",
        "Compute mean and standard deviation of accuracy across seeds for each (train → val) pair.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Summary across seeds ===\")\n",
        "for tp in train_paths:\n",
        "    for vp in val_paths:\n",
        "        arr = np.array(results[tp][vp])\n",
        "        mean, std = arr.mean(), arr.std(ddof=1)\n",
        "        print(f\"{tp} -> {vp}: Mean = {mean:.2f}%, Std = {std:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hiwi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
