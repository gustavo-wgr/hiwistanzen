{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Transfer Learning Pipeline\n",
        "\n",
        "This notebook pre-trains a model on a primary dataset split, then fine-tunes on multiple transfer sets, evaluating on several validation sets across multiple seeds. Results are summarized as mean ± std for each transfer → validation pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Setup\n",
        "\n",
        "Load necessary libraries, dataset modules, and helper functions. Configure device and loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, random\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from pkldataset import PKLDataset\n",
        "from helpers import set_seed, get_model, train_model, eval_model\n",
        "\n",
        "# Device and loss\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "- **Primary pretraining path**: path to dataset folder to split  \n",
        "- **Transfer sets**: list of datasets for fine-tuning  \n",
        "- **Validation sets**: list of held-out datasets for evaluation  \n",
        "- **Seeds**: for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primary dataset path (will be split)\n",
        "train_path_1 = r\"C:\\Users\\gus07\\Desktop\\data hiwi\\preprocessing\\HC\\T197\\RP\"\n",
        "\n",
        "# Transfer learning datasets\n",
        "transfer_sets = [\n",
        "    \"../datasets/RPDC197/train_20\",\n",
        "    \"../datasets/RPDC197/train_50\",\n",
        "    \"../datasets/RPDC197/train_100\",\n",
        "    \"../datasets/RPDC197/train_200\",\n",
        "    \"../datasets/RPDC197/train_300\",\n",
        "    \"../datasets/RPDC197/train_400\",\n",
        "    \"../datasets/RPDC197/train_500\",\n",
        "    \"../datasets/RPDC197/train_600\",\n",
        "]\n",
        "\n",
        "# Validation sets\n",
        "val_paths = [\n",
        "    \"../datasets/RPDC185/val_1000\",\n",
        "    \"../datasets/RPDC188/val_1000\",\n",
        "    \"../datasets/RPDC191/val_1000\",\n",
        "    \"../datasets/RPDC194/val_1000\",\n",
        "    \"../datasets/RPDC197/val_1000\",\n",
        "]\n",
        "\n",
        "# Random seeds\n",
        "seeds = [101,202,303,404,505,606,707,808,909,1001]\n",
        "\n",
        "# Prepare results container\n",
        "results = {t: {vp: [] for vp in val_paths} for t in transfer_sets}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Phase 1: Pretraining on Source Dataset (HC)\n",
        "\n",
        "For each seed:\n",
        "1. Split the primary dataset into train/val\n",
        "2. Train for 10 epochs\n",
        "3. Save pretrained weights\n",
        "### 4. Phase 2: Fine-Tuning on Transfer Sets\n",
        "\n",
        "For each seed and each transfer set:\n",
        "1. Load pretrained weights\n",
        "2. Train for 100 epochs\n",
        "3. Evaluate on validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for seed in seeds:\n",
        "    print(f\"\\n>>> Seed {seed} - Pretraining\")\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Split dataset\n",
        "    train_ds, val_ds = PKLDataset.split_dataset(train_path_1)\n",
        "    train_loader1 = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    \n",
        "\n",
        "    model = get_model().to(device)\n",
        "    opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "    sch = optim.lr_scheduler.StepLR(opt, step_size=50, gamma=0.1)\n",
        "\n",
        "    model = train_model(model, train_loader1, criterion, opt, sch, num_epochs=10, device=device)\n",
        "    pretrained_state = model.state_dict()\n",
        "    # Transfer learning phase\n",
        "    for t in transfer_sets:\n",
        "        print(f\"--- Transfer on {t}\")\n",
        "        tl_model = get_model().to(device)\n",
        "        tl_model.load_state_dict(pretrained_state)\n",
        "\n",
        "        loader_t = DataLoader(PKLDataset(t), batch_size=64, shuffle=True)\n",
        "        opt2 = optim.Adam(tl_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "        sch2 = optim.lr_scheduler.StepLR(opt2, step_size=25, gamma=0.1)\n",
        "\n",
        "        tl_model = train_model(tl_model, loader_t, criterion, opt2, sch2, num_epochs=100, device=device)\n",
        "\n",
        "        # Evaluate on validation sets\n",
        "        for vp in val_paths:\n",
        "            acc = eval_model(tl_model, DataLoader(PKLDataset(vp), batch_size=64, shuffle=False), device)\n",
        "            results[t][vp].append(acc)\n",
        "            print(f\"[Seed {seed}] {t} → {vp}: {acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary of Results\n",
        "\n",
        "Compute mean and standard deviation across seeds for each transfer → validation pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Mean ± Std Dev over seeds ===\")\n",
        "for t in transfer_sets:\n",
        "    for vp in val_paths:\n",
        "        arr = np.array(results[t][vp])\n",
        "        mean_acc = arr.mean()\n",
        "        std_acc = arr.std(ddof=1)\n",
        "        print(f\"{t} → {vp}: mean = {mean_acc:.2f}%,  std = {std_acc:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hiwi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
