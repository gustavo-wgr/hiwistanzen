{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fdaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source (phase1) val-acc: 99.30%\n",
      "\n",
      ">>> Adaptation seed 101\n",
      "  [IM] Epoch 1/100  H_cond=0.1773  H_marg=2.0991\n",
      "  [IM] Epoch 20/100  H_cond=0.0476  H_marg=2.2159\n",
      "  [IM] Epoch 40/100  H_cond=0.0337  H_marg=2.2428\n",
      "  [PL] Epoch 51/100  Pseudo‐CE Loss: 0.2762\n",
      "  [PL] Epoch 60/100  Pseudo‐CE Loss: 0.2229\n",
      "  [PL] Epoch 80/100  Pseudo‐CE Loss: 0.0623\n",
      "  [PL] Epoch 100/100  Pseudo‐CE Loss: 0.0884\n",
      "  → ../datasets/RPDC185/val_100: 95.30%\n",
      "  → ../datasets/RPDC188/val_1000: 98.80%\n",
      "  → ../datasets/RPDC191/val_1000: 91.91%\n",
      "  → ../datasets/RPDC194/val_1000: 88.30%\n",
      "  → ../datasets/RPDC197/val_1000: 88.30%\n",
      "\n",
      ">>> Adaptation seed 202\n",
      "  [IM] Epoch 1/100  H_cond=0.1871  H_marg=2.0612\n",
      "  [IM] Epoch 20/100  H_cond=0.0629  H_marg=2.2284\n",
      "  [IM] Epoch 40/100  H_cond=0.0253  H_marg=2.2397\n",
      "  [PL] Epoch 51/100  Pseudo‐CE Loss: 0.1884\n",
      "  [PL] Epoch 60/100  Pseudo‐CE Loss: 0.0966\n",
      "  [PL] Epoch 80/100  Pseudo‐CE Loss: 0.0575\n",
      "  [PL] Epoch 100/100  Pseudo‐CE Loss: 0.0305\n",
      "  → ../datasets/RPDC185/val_100: 95.50%\n",
      "  → ../datasets/RPDC188/val_1000: 98.93%\n",
      "  → ../datasets/RPDC191/val_1000: 92.03%\n",
      "  → ../datasets/RPDC194/val_1000: 87.70%\n",
      "  → ../datasets/RPDC197/val_1000: 86.80%\n",
      "\n",
      ">>> Adaptation seed 303\n",
      "  [IM] Epoch 1/100  H_cond=0.1955  H_marg=2.0718\n",
      "  [IM] Epoch 20/100  H_cond=0.0442  H_marg=2.2128\n",
      "  [IM] Epoch 40/100  H_cond=0.0309  H_marg=2.2406\n",
      "  [PL] Epoch 51/100  Pseudo‐CE Loss: 0.1062\n",
      "  [PL] Epoch 60/100  Pseudo‐CE Loss: 0.0719\n",
      "  [PL] Epoch 80/100  Pseudo‐CE Loss: 0.0575\n",
      "  [PL] Epoch 100/100  Pseudo‐CE Loss: 0.0379\n",
      "  → ../datasets/RPDC185/val_100: 95.60%\n",
      "  → ../datasets/RPDC188/val_1000: 98.66%\n",
      "  → ../datasets/RPDC191/val_1000: 90.43%\n",
      "  → ../datasets/RPDC194/val_1000: 85.60%\n",
      "  → ../datasets/RPDC197/val_1000: 85.90%\n",
      "\n",
      ">>> Adaptation seed 404\n",
      "  [IM] Epoch 1/100  H_cond=0.1794  H_marg=2.0661\n",
      "  [IM] Epoch 20/100  H_cond=0.0553  H_marg=2.2277\n",
      "  [IM] Epoch 40/100  H_cond=0.0305  H_marg=2.2346\n",
      "  [PL] Epoch 51/100  Pseudo‐CE Loss: 0.2359\n",
      "  [PL] Epoch 60/100  Pseudo‐CE Loss: 0.0988\n",
      "  [PL] Epoch 80/100  Pseudo‐CE Loss: 0.0438\n",
      "  [PL] Epoch 100/100  Pseudo‐CE Loss: 0.0498\n",
      "  → ../datasets/RPDC185/val_100: 96.00%\n",
      "  → ../datasets/RPDC188/val_1000: 98.53%\n",
      "  → ../datasets/RPDC191/val_1000: 90.55%\n",
      "  → ../datasets/RPDC194/val_1000: 86.80%\n",
      "  → ../datasets/RPDC197/val_1000: 86.80%\n",
      "\n",
      ">>> Adaptation seed 505\n",
      "  [IM] Epoch 1/100  H_cond=0.1887  H_marg=2.0940\n",
      "  [IM] Epoch 20/100  H_cond=0.0517  H_marg=2.2494\n",
      "  [IM] Epoch 40/100  H_cond=0.0250  H_marg=2.2352\n",
      "  [PL] Epoch 51/100  Pseudo‐CE Loss: 0.2283\n",
      "  [PL] Epoch 60/100  Pseudo‐CE Loss: 0.0719\n",
      "  [PL] Epoch 80/100  Pseudo‐CE Loss: 0.0532\n",
      "  [PL] Epoch 100/100  Pseudo‐CE Loss: 0.0813\n",
      "  → ../datasets/RPDC185/val_100: 96.90%\n",
      "  → ../datasets/RPDC188/val_1000: 99.06%\n",
      "  → ../datasets/RPDC191/val_1000: 89.86%\n",
      "  → ../datasets/RPDC194/val_1000: 85.60%\n",
      "  → ../datasets/RPDC197/val_1000: 84.80%\n",
      "\n",
      ">>> Adaptation seed 606\n",
      "  [IM] Epoch 1/100  H_cond=0.1790  H_marg=2.0947\n",
      "  [IM] Epoch 20/100  H_cond=0.0451  H_marg=2.2232\n",
      "  [IM] Epoch 40/100  H_cond=0.0300  H_marg=2.2383\n",
      "  [PL] Epoch 51/100  Pseudo‐CE Loss: 0.2309\n",
      "  [PL] Epoch 60/100  Pseudo‐CE Loss: 0.0864\n",
      "  [PL] Epoch 80/100  Pseudo‐CE Loss: 0.0354\n",
      "  [PL] Epoch 100/100  Pseudo‐CE Loss: 0.0408\n",
      "  → ../datasets/RPDC185/val_100: 95.90%\n",
      "  → ../datasets/RPDC188/val_1000: 98.80%\n",
      "  → ../datasets/RPDC191/val_1000: 90.43%\n",
      "  → ../datasets/RPDC194/val_1000: 85.90%\n",
      "  → ../datasets/RPDC197/val_1000: 86.00%\n",
      "\n",
      ">>> Adaptation seed 707\n",
      "  [IM] Epoch 1/100  H_cond=0.1940  H_marg=2.0679\n",
      "  [IM] Epoch 20/100  H_cond=0.0576  H_marg=2.2393\n",
      "  [IM] Epoch 40/100  H_cond=0.0402  H_marg=2.2450\n",
      "  [PL] Epoch 51/100  Pseudo‐CE Loss: 0.1773\n",
      "  [PL] Epoch 60/100  Pseudo‐CE Loss: 0.1143\n",
      "  [PL] Epoch 80/100  Pseudo‐CE Loss: 0.0986\n",
      "  [PL] Epoch 100/100  Pseudo‐CE Loss: 0.0605\n",
      "  → ../datasets/RPDC185/val_100: 95.90%\n",
      "  → ../datasets/RPDC188/val_1000: 98.80%\n",
      "  → ../datasets/RPDC191/val_1000: 89.98%\n",
      "  → ../datasets/RPDC194/val_1000: 85.60%\n",
      "  → ../datasets/RPDC197/val_1000: 85.60%\n",
      "\n",
      ">>> Adaptation seed 808\n",
      "  [IM] Epoch 1/100  H_cond=0.2056  H_marg=2.1011\n",
      "  [IM] Epoch 20/100  H_cond=0.0342  H_marg=2.2379\n",
      "  [IM] Epoch 40/100  H_cond=0.0236  H_marg=2.2549\n",
      "  [PL] Epoch 51/100  Pseudo‐CE Loss: 0.2072\n",
      "  [PL] Epoch 60/100  Pseudo‐CE Loss: 0.0691\n",
      "  [PL] Epoch 80/100  Pseudo‐CE Loss: 0.0868\n",
      "  [PL] Epoch 100/100  Pseudo‐CE Loss: 0.0499\n",
      "  → ../datasets/RPDC185/val_100: 96.90%\n",
      "  → ../datasets/RPDC188/val_1000: 98.93%\n",
      "  → ../datasets/RPDC191/val_1000: 90.55%\n",
      "  → ../datasets/RPDC194/val_1000: 85.30%\n",
      "  → ../datasets/RPDC197/val_1000: 85.20%\n",
      "\n",
      ">>> Adaptation seed 909\n",
      "  [IM] Epoch 1/100  H_cond=0.1690  H_marg=2.0866\n",
      "  [IM] Epoch 20/100  H_cond=0.0536  H_marg=2.2220\n",
      "  [IM] Epoch 40/100  H_cond=0.0371  H_marg=2.2381\n",
      "  [PL] Epoch 51/100  Pseudo‐CE Loss: 0.1808\n",
      "  [PL] Epoch 60/100  Pseudo‐CE Loss: 0.0947\n",
      "  [PL] Epoch 80/100  Pseudo‐CE Loss: 0.0283\n",
      "  [PL] Epoch 100/100  Pseudo‐CE Loss: 0.0482\n",
      "  → ../datasets/RPDC185/val_100: 96.80%\n",
      "  → ../datasets/RPDC188/val_1000: 98.80%\n",
      "  → ../datasets/RPDC191/val_1000: 89.86%\n",
      "  → ../datasets/RPDC194/val_1000: 85.60%\n",
      "  → ../datasets/RPDC197/val_1000: 85.20%\n",
      "\n",
      ">>> Adaptation seed 1001\n",
      "  [IM] Epoch 1/100  H_cond=0.1999  H_marg=2.0652\n",
      "  [IM] Epoch 20/100  H_cond=0.0555  H_marg=2.2307\n",
      "  [IM] Epoch 40/100  H_cond=0.0405  H_marg=2.2427\n",
      "  [PL] Epoch 51/100  Pseudo‐CE Loss: 0.1187\n",
      "  [PL] Epoch 60/100  Pseudo‐CE Loss: 0.0508\n",
      "  [PL] Epoch 80/100  Pseudo‐CE Loss: 0.0471\n",
      "  [PL] Epoch 100/100  Pseudo‐CE Loss: 0.0656\n",
      "  → ../datasets/RPDC185/val_100: 96.10%\n",
      "  → ../datasets/RPDC188/val_1000: 98.93%\n",
      "  → ../datasets/RPDC191/val_1000: 91.00%\n",
      "  → ../datasets/RPDC194/val_1000: 86.70%\n",
      "  → ../datasets/RPDC197/val_1000: 85.40%\n",
      "\n",
      "=== Mean ± Std Dev over seeds ===\n",
      "../datasets/RPDC185/train_500 → ../datasets/RPDC185/val_100: mean=96.09%, std=0.59%\n",
      "../datasets/RPDC185/train_500 → ../datasets/RPDC188/val_1000: mean=98.82%, std=0.15%\n",
      "../datasets/RPDC185/train_500 → ../datasets/RPDC191/val_1000: mean=90.66%, std=0.78%\n",
      "../datasets/RPDC185/train_500 → ../datasets/RPDC194/val_1000: mean=86.31%, std=1.03%\n",
      "../datasets/RPDC185/train_500 → ../datasets/RPDC197/val_1000: mean=86.00%, std=1.04%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pkldataset import PKLDataset\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark   = False\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_length: int = 2800, num_classes: int = 10, input_channels: int = 1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 16, kernel_size=31, padding=15),\n",
    "            nn.BatchNorm1d(16), nn.ReLU(inplace=True), nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=31, padding=15),\n",
    "            nn.BatchNorm1d(32), nn.ReLU(inplace=True), nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=31, padding=15),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(inplace=True), nn.MaxPool1d(2)\n",
    "        )\n",
    "        conv_output_length = input_length // 8\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * conv_output_length, 128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier_head(x)\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler,\n",
    "                num_epochs, device, max_grad_norm=1.0):\n",
    "    best_loss, best_state = float('inf'), None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if targets.dim() > 1:\n",
    "                targets = targets.argmax(dim=1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(inputs), targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            running += loss.item() * inputs.size(0)\n",
    "        scheduler.step()\n",
    "        epoch_loss = running / len(train_loader.dataset)\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss, best_state = epoch_loss, model.state_dict()\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def eval_model(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if targets.dim() > 1:\n",
    "                targets = targets.argmax(dim=1)\n",
    "            _, pred = model(inputs).max(1)\n",
    "            total   += targets.size(0)\n",
    "            correct += (pred == targets).sum().item()\n",
    "    return 100. * correct / total\n",
    "\n",
    "# === CONFIG ===\n",
    "train_path_1  = r\"C:\\Users\\gus07\\Desktop\\data hiwi\\preprocessing\\DC\\T197\\RP\"\n",
    "transfer_sets = [\"../datasets/RPDC185/train_500\"]\n",
    "val_paths = [\n",
    "    \"../datasets/RPDC185/val_100\",\n",
    "    \"../datasets/RPDC188/val_1000\",\n",
    "    \"../datasets/RPDC191/val_1000\",\n",
    "    \"../datasets/RPDC194/val_1000\",\n",
    "    \"../datasets/RPDC197/val_1000\",\n",
    "]\n",
    "seeds         = [101, 202, 303, 404, 505, 606, 707, 808, 909, 1001]\n",
    "device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion     = nn.CrossEntropyLoss()\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "\n",
    "# 1) ONE‐TIME SOURCE TRAINING (fixed seed)\n",
    "# ──────────────────────────────────────────────\n",
    "seed0 = 42\n",
    "set_seed(seed0)\n",
    "# split & loaders\n",
    "train_ds, val_ds = PKLDataset.split_dataset(train_path_1)\n",
    "loader_tr = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "loader_va = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "# instantiate, train, evaluate\n",
    "source_model = CNN().to(device)\n",
    "opt0   = optim.Adam(source_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "sch0   = optim.lr_scheduler.StepLR(opt0, step_size=50, gamma=0.1)\n",
    "source_model = train_model(source_model, loader_tr, criterion, opt0, sch0, num_epochs=10, device=device)\n",
    "print(f\"Source (phase1) val-acc: {eval_model(source_model, loader_va, device):.2f}%\")\n",
    "\n",
    "# save once\n",
    "pretrained_state = source_model.state_dict()\n",
    "\n",
    "\n",
    "results = {t:{vp:[] for vp in val_paths} for t in transfer_sets}\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n>>> Adaptation seed {seed}\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    for t in transfer_sets:\n",
    "        # reload student from the single pretrained source\n",
    "        student = CNN().to(device)\n",
    "        student.load_state_dict(pretrained_state)\n",
    "        student.train()\n",
    "\n",
    "        # Freeze head initially\n",
    "        for p in student.classifier_head.parameters():\n",
    "            p.requires_grad = False\n",
    "        feat_params = (\n",
    "            list(student.conv1.parameters()) +\n",
    "            list(student.conv2.parameters()) +\n",
    "            list(student.conv3.parameters()) +\n",
    "            list(student.feature_extractor.parameters())\n",
    "        )\n",
    "\n",
    "        shot_optimizer = optim.Adam(feat_params, lr=1e-4, weight_decay=1e-5)\n",
    "        # smooth cosine decay over ALL epochs\n",
    "        total_epochs   = 100\n",
    "        shot_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            shot_optimizer, T_max=total_epochs, eta_min=1e-6\n",
    "        )\n",
    "\n",
    "        lambda_shot    = 1.0\n",
    "        im_only        = 50\n",
    "        pseudo_labels_all = None\n",
    "        pseudo_confidence = None\n",
    "\n",
    "        dataset_t      = PKLDataset(t)\n",
    "        unlab_loader   = DataLoader(dataset_t, batch_size=64, shuffle=True)\n",
    "        full_loader_t  = DataLoader(dataset_t, batch_size=64, shuffle=False)\n",
    "\n",
    "        for epoch in range(total_epochs):\n",
    "            student.train()\n",
    "\n",
    "            # --- Information Maximization ---\n",
    "            if epoch < im_only:\n",
    "                running_Hc = running_Hm = 0.0\n",
    "                for x, _ in unlab_loader:\n",
    "                    x = x.to(device)\n",
    "                    logits = student(x)\n",
    "                    probs  = torch.softmax(logits, dim=1)\n",
    "                    logp   = torch.log(probs + 1e-12)\n",
    "                    Hc     = -(probs * logp).sum(1).mean()\n",
    "                    p_bar  = probs.mean(0)\n",
    "                    Hm     = -(p_bar * torch.log(p_bar + 1e-12)).sum()\n",
    "                    loss = Hc - lambda_shot * Hm\n",
    "\n",
    "                    shot_optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(feat_params, 1.0)\n",
    "                    shot_optimizer.step()\n",
    "\n",
    "                    running_Hc += Hc.item() * x.size(0)\n",
    "                    running_Hm += Hm.item() * x.size(0)\n",
    "\n",
    "                if (epoch+1)%20==0 or epoch==0:\n",
    "                    N = len(unlab_loader.dataset)\n",
    "                    print(f\"  [IM] Epoch {epoch+1}/{total_epochs}  \"\n",
    "                          f\"H_cond={(running_Hc/N):.4f}  H_marg={(running_Hm/N):.4f}\")\n",
    "\n",
    "                shot_scheduler.step()\n",
    "\n",
    "                # compute pseudo‐labels at end of IM\n",
    "                if epoch == im_only - 1:\n",
    "                    student.eval()\n",
    "                    feats, probs_all = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for x, _ in full_loader_t:\n",
    "                            x = x.to(device)\n",
    "                            f = student.extract_features(x)\n",
    "                            p = torch.softmax(student.classifier_head(f), dim=1)\n",
    "                            feats.append(f.cpu())\n",
    "                            probs_all.append(p.cpu())\n",
    "                    feats = torch.cat(feats)\n",
    "                    probs_all = torch.cat(probs_all)\n",
    "\n",
    "                    # centroids & hard assignments\n",
    "                    C = probs_all.size(1)\n",
    "                    centroids = torch.zeros(C, feats.size(1))\n",
    "                    for k in range(C):\n",
    "                        w = probs_all[:,k].unsqueeze(1)\n",
    "                        denom = w.sum()\n",
    "                        if denom>0:\n",
    "                            centroids[k] = (w*feats).sum(0)/denom\n",
    "                    dists = torch.cdist(feats, centroids)\n",
    "                    pseudo_labels_all  = torch.argmin(dists, dim=1)\n",
    "                    pseudo_confidence = probs_all.max(1).values\n",
    "\n",
    "                    # unfreeze head & add it to optimizer with lower LR\n",
    "                    for p in student.classifier_head.parameters():\n",
    "                        p.requires_grad = True\n",
    "                    shot_optimizer.add_param_group({\n",
    "                        'params': student.classifier_head.parameters(),\n",
    "                        'lr': shot_optimizer.param_groups[0]['lr'] * 0.2,\n",
    "                        'weight_decay': 1e-5\n",
    "                    })\n",
    "                    student.train()\n",
    "\n",
    "            # --- Pseudo‐label Fine‐tuning ---\n",
    "            else:\n",
    "                if pseudo_labels_all is None:\n",
    "                    raise RuntimeError(\"No pseudo-labels!\")\n",
    "                running_pl = 0.0\n",
    "                # shuffle indices each epoch\n",
    "                idxs = torch.randperm(len(dataset_t)).tolist()\n",
    "                pl_subset = Subset(dataset_t, idxs)\n",
    "                pl_loader = DataLoader(pl_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "                for b_idx, (x, _) in enumerate(pl_loader):\n",
    "                    start = b_idx * pl_loader.batch_size\n",
    "                    end   = start + x.size(0)\n",
    "                    batch_idx = idxs[start:end]\n",
    "                    conf = pseudo_confidence[batch_idx].to(device)\n",
    "                    labels = pseudo_labels_all[batch_idx].to(device)\n",
    "\n",
    "                    x = x.to(device)\n",
    "                    f = student.extract_features(x)\n",
    "                    logits = student.classifier_head(f)\n",
    "\n",
    "                    # confidence‐weighted loss\n",
    "                    per_sample = nn.CrossEntropyLoss(reduction='none')(logits, labels)\n",
    "                    # optional thresholding to skip very low‐conf samples\n",
    "                    mask = conf >= 0.3\n",
    "                    if mask.sum()>0:\n",
    "                        loss = (per_sample[mask] * conf[mask]).mean()\n",
    "                    else:\n",
    "                        loss = per_sample.mean()\n",
    "\n",
    "                    shot_optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(shot_optimizer.param_groups[0]['params'], 1.0)\n",
    "                    shot_optimizer.step()\n",
    "\n",
    "                    running_pl += loss.item() * x.size(0)\n",
    "\n",
    "                if (epoch+1)%20==0 or epoch==im_only:\n",
    "                    N = len(dataset_t)\n",
    "                    print(f\"  [PL] Epoch {epoch+1}/{total_epochs}  Pseudo‐CE Loss: {(running_pl/N):.4f}\")\n",
    "\n",
    "                shot_scheduler.step()\n",
    "\n",
    "        # evaluate on each target val set\n",
    "        student.eval()\n",
    "        for vp in val_paths:\n",
    "            vl = DataLoader(PKLDataset(vp), batch_size=64, shuffle=False)\n",
    "            acc = eval_model(student, vl, device)\n",
    "            results[t][vp].append(acc)\n",
    "            print(f\"  → {vp}: {acc:.2f}%\")\n",
    "\n",
    "# final summary\n",
    "print(\"\\n=== Mean ± Std Dev over seeds ===\")\n",
    "for t in transfer_sets:\n",
    "    for vp in val_paths:\n",
    "        arr = np.array(results[t][vp])\n",
    "        print(f\"{t} → {vp}: mean={arr.mean():.2f}%, std={arr.std(ddof=1):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiwi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
