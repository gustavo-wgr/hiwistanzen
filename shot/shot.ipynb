{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SHOT vs Coral Pipeline\n",
        "\n",
        "This notebook pre-trains a CNN on a primary dataset split, then adapts to a target domain with Information Maximization followed by Pseudo-Label fine-tuning across multiple seeds. Evaluations on several validation sets are recorded and summarized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Model Definition\n",
        "\n",
        "Load libraries, define the CNN architecture, and import helpers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "from pkldataset import PKLDataset\n",
        "from helpers import train_model, eval_model, set_seed\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_length: int = 2800, num_classes: int = 10, input_channels: int = 1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 16, kernel_size=31, padding=15),\n",
        "            nn.BatchNorm1d(16), nn.ReLU(inplace=True), nn.MaxPool1d(2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv1d(16, 32, kernel_size=31, padding=15),\n",
        "            nn.BatchNorm1d(32), nn.ReLU(inplace=True), nn.MaxPool1d(2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv1d(32, 64, kernel_size=31, padding=15),\n",
        "            nn.BatchNorm1d(64), nn.ReLU(inplace=True), nn.MaxPool1d(2)\n",
        "        )\n",
        "        conv_output_length = input_length // 8\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * conv_output_length, 128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.feature_extractor(x)\n",
        "        return self.classifier_head(x)\n",
        "\n",
        "    def extract_features(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        return self.feature_extractor(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "- **Primary dataset**: path to pretraining folder  \n",
        "- **Transfer set(s)**: domain(s) for adaptation  \n",
        "- **Validation sets**: held-out datasets for evaluation  \n",
        "- **Seeds**: random seeds for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primary source path (will be split)\n",
        "train_path_1 = r\"C:\\Users\\gus07\\Desktop\\data hiwi\\preprocessing\\HC\\T197\\RP\"\n",
        "\n",
        "# Target adaptation sets\n",
        "transfer_sets = [\"../datasets/RPDC185/train_500\"]\n",
        "\n",
        "# Validation sets\n",
        "val_paths = [\n",
        "    \"../datasets/RPDC185/val_1000\",\n",
        "    \"../datasets/RPDC188/val_1000\",\n",
        "    \"../datasets/RPDC191/val_1000\",\n",
        "    \"../datasets/RPDC194/val_1000\",\n",
        "    \"../datasets/RPDC197/val_1000\",\n",
        "]\n",
        "\n",
        "# Random seeds\n",
        "seeds = [101, 202, 303, 404, 505, 606, 707, 808, 909, 1001]\n",
        "\n",
        "# Device and loss\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Results container\n",
        "results = {t: {vp: [] for vp in val_paths} for t in transfer_sets}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Phase 1: Source Pretraining\n",
        "\n",
        "Train on the primary dataset split once with a fixed seed, save pretrained weights, and evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-time source training\n",
        "seed0 = 42\n",
        "set_seed(seed0)\n",
        "\n",
        "# Split source dataset\n",
        "train_ds, val_ds = PKLDataset.split_dataset(train_path_1)\n",
        "loader_tr = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "loader_va = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize and train\n",
        "source_model = CNN().to(device)\n",
        "opt0 = optim.Adam(source_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "sch0 = optim.lr_scheduler.StepLR(opt0, step_size=50, gamma=0.1)\n",
        "source_model = train_model(\n",
        "    source_model, loader_tr, criterion, opt0, sch0,\n",
        "    num_epochs=10, device=device\n",
        ")\n",
        "print(f\"Source (phase1) val-acc: {eval_model(source_model, loader_va, device):.2f}%\")\n",
        "\n",
        "# Save pretrained state\n",
        "pretrained_state = source_model.state_dict()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Phase 2: Self-Supervised Adaptation\n",
        "\n",
        "For each seed and transfer set:\n",
        "1. Load pretrained model\n",
        "2. Run Information Maximization for `im_only` epochs\n",
        "3. Generate pseudo-labels and unfreeze head\n",
        "4. Fine-tune with pseudo-label loss for remaining epochs\n",
        "5. Evaluate on validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for seed in seeds:\n",
        "    print(f\"\\n>>> Adaptation seed {seed}\")\n",
        "    set_seed(seed)\n",
        "\n",
        "    for t in transfer_sets:\n",
        "        # Reload from pretrained\n",
        "        student = CNN().to(device)\n",
        "        student.load_state_dict(pretrained_state)\n",
        "        student.train()\n",
        "\n",
        "        # Freeze head\n",
        "        for p in student.classifier_head.parameters():\n",
        "            p.requires_grad = False\n",
        "        feat_params = list(student.conv1.parameters()) + list(student.conv2.parameters()) + \\\n",
        "                      list(student.conv3.parameters()) + list(student.feature_extractor.parameters())\n",
        "\n",
        "        shot_optimizer = optim.Adam(feat_params, lr=1e-4, weight_decay=1e-5)\n",
        "        total_epochs = 100\n",
        "        shot_scheduler = optim.lr_scheduler.CosineAnnealingLR(shot_optimizer, T_max=total_epochs, eta_min=1e-6)\n",
        "\n",
        "        # Information Maximization settings\n",
        "        lambda_shot = 1.0\n",
        "        im_only = 50\n",
        "        pseudo_labels_all = None\n",
        "        pseudo_confidence = None\n",
        "\n",
        "        dataset_t = PKLDataset(t)\n",
        "        unlab_loader = DataLoader(dataset_t, batch_size=64, shuffle=True)\n",
        "        full_loader_t = DataLoader(dataset_t, batch_size=64, shuffle=False)\n",
        "\n",
        "        for epoch in range(total_epochs):\n",
        "            student.train()\n",
        "            # Information Maximization phase\n",
        "            if epoch < im_only:\n",
        "                running_Hc = running_Hm = 0.0\n",
        "                for x, _ in unlab_loader:\n",
        "                    x = x.to(device)\n",
        "                    logits = student(x)\n",
        "                    probs = torch.softmax(logits, dim=1)\n",
        "                    logp = torch.log(probs + 1e-12)\n",
        "                    Hc = -(probs * logp).sum(1).mean()\n",
        "                    p_bar = probs.mean(0)\n",
        "                    Hm = -(p_bar * torch.log(p_bar + 1e-12)).sum()\n",
        "                    loss = Hc - lambda_shot * Hm\n",
        "\n",
        "                    shot_optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(feat_params, 1.0)\n",
        "                    shot_optimizer.step()\n",
        "\n",
        "                    running_Hc += Hc.item() * x.size(0)\n",
        "                    running_Hm += Hm.item() * x.size(0)\n",
        "\n",
        "                if (epoch+1) % 20 == 0 or epoch == 0:\n",
        "                    N = len(unlab_loader.dataset)\n",
        "                    print(f\"  [IM] Epoch {epoch+1}/{total_epochs}  H_cond={(running_Hc/N):.4f}  H_marg={(running_Hm/N):.4f}\")\n",
        "\n",
        "                shot_scheduler.step()\n",
        "\n",
        "                # Generate pseudo-labels at end of IM\n",
        "                if epoch == im_only - 1:\n",
        "                    student.eval()\n",
        "                    feats, probs_all = [], []\n",
        "                    with torch.no_grad():\n",
        "                        for x, _ in full_loader_t:\n",
        "                            x = x.to(device)\n",
        "                            f = student.extract_features(x)\n",
        "                            p = torch.softmax(student.classifier_head(f), dim=1)\n",
        "                            feats.append(f.cpu())\n",
        "                            probs_all.append(p.cpu())\n",
        "                    feats = torch.cat(feats)\n",
        "                    probs_all = torch.cat(probs_all)\n",
        "\n",
        "                    C = probs_all.size(1)\n",
        "                    centroids = torch.zeros(C, feats.size(1))\n",
        "                    for k in range(C):\n",
        "                        w = probs_all[:, k].unsqueeze(1)\n",
        "                        denom = w.sum()\n",
        "                        if denom > 0:\n",
        "                            centroids[k] = (w * feats).sum(0) / denom\n",
        "                    dists = torch.cdist(feats, centroids)\n",
        "                    pseudo_labels_all = torch.argmin(dists, dim=1)\n",
        "                    pseudo_confidence = probs_all.max(1).values\n",
        "\n",
        "                    # Unfreeze head and add to optimizer\n",
        "                    for p in student.classifier_head.parameters():\n",
        "                        p.requires_grad = True\n",
        "                    shot_optimizer.add_param_group({\n",
        "                        'params': student.classifier_head.parameters(),\n",
        "                        'lr': shot_optimizer.param_groups[0]['lr'] * 0.2,\n",
        "                        'weight_decay': 1e-5\n",
        "                    })\n",
        "                    student.train()\n",
        "            # Pseudo-Label fine-tuning phase\n",
        "            else:\n",
        "                if pseudo_labels_all is None:\n",
        "                    raise RuntimeError(\"No pseudo-labels!\")\n",
        "                running_pl = 0.0\n",
        "                idxs = torch.randperm(len(dataset_t)).tolist()\n",
        "                pl_subset = Subset(dataset_t, idxs)\n",
        "                pl_loader = DataLoader(pl_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "                for b_idx, (x, _) in enumerate(pl_loader):\n",
        "                    start = b_idx * pl_loader.batch_size\n",
        "                    end = start + x.size(0)\n",
        "                    batch_idx = idxs[start:end]\n",
        "                    conf = pseudo_confidence[batch_idx].to(device)\n",
        "                    labels = pseudo_labels_all[batch_idx].to(device)\n",
        "\n",
        "                    x = x.to(device)\n",
        "                    f = student.extract_features(x)\n",
        "                    logits = student.classifier_head(f)\n",
        "\n",
        "                    per_sample = nn.CrossEntropyLoss(reduction='none')(logits, labels)\n",
        "                    mask = conf >= 0.3\n",
        "                    loss = (per_sample[mask] * conf[mask]).mean() if mask.sum() > 0 else per_sample.mean()\n",
        "\n",
        "                    shot_optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(shot_optimizer.param_groups[0]['params'], 1.0)\n",
        "                    shot_optimizer.step()\n",
        "\n",
        "                    running_pl += loss.item() * x.size(0)\n",
        "\n",
        "                if (epoch+1) % 20 == 0 or epoch == im_only:\n",
        "                    N = len(dataset_t)\n",
        "                    print(f\"  [PL] Epoch {epoch+1}/{total_epochs}  Pseudo-CE Loss: {(running_pl/N):.4f}\")\n",
        "\n",
        "                shot_scheduler.step()\n",
        "\n",
        "        # Evaluation on validation sets\n",
        "        student.eval()\n",
        "        for vp in val_paths:\n",
        "            vl = DataLoader(PKLDataset(vp), batch_size=64, shuffle=False)\n",
        "            acc = eval_model(student, vl, device)\n",
        "            results[t][vp].append(acc)\n",
        "            print(f\"  → {vp}: {acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Final Summary\n",
        "\n",
        "Compute mean and standard deviation of accuracy across seeds for each transfer → validation pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Mean ± Std Dev over seeds ===\")\n",
        "for t in transfer_sets:\n",
        "    for vp in val_paths:\n",
        "        arr = np.array(results[t][vp])\n",
        "        print(f\"{t} → {vp}: mean={arr.mean():.2f}%, std={arr.std(ddof=1):.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hiwi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
