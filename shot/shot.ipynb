{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fdaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pkldataset import PKLDataset\n",
    "from helpers import train_model, eval_model, set_seed\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_length: int = 2800, num_classes: int = 10, input_channels: int = 1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 16, kernel_size=31, padding=15),\n",
    "            nn.BatchNorm1d(16), nn.ReLU(inplace=True), nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=31, padding=15),\n",
    "            nn.BatchNorm1d(32), nn.ReLU(inplace=True), nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=31, padding=15),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(inplace=True), nn.MaxPool1d(2)\n",
    "        )\n",
    "        conv_output_length = input_length // 8\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * conv_output_length, 128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier_head(x)\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "\n",
    "# === CONFIG ===\n",
    "train_path_1  = r\"C:\\Users\\gus07\\Desktop\\data hiwi\\preprocessing\\HC\\T197\\RP\"\n",
    "transfer_sets = [\"../datasets/RPDC185/train_500\"]\n",
    "val_paths = [\n",
    "    \"../datasets/RPDC185/val_1000\",\n",
    "    \"../datasets/RPDC188/val_1000\",\n",
    "    \"../datasets/RPDC191/val_1000\",\n",
    "    \"../datasets/RPDC194/val_1000\",\n",
    "    \"../datasets/RPDC197/val_1000\",\n",
    "]\n",
    "seeds         = [101, 202, 303, 404, 505, 606, 707, 808, 909, 1001]\n",
    "device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion     = nn.CrossEntropyLoss()\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "\n",
    "# 1) ONE‐TIME SOURCE TRAINING (fixed seed)\n",
    "# ──────────────────────────────────────────────\n",
    "seed0 = 42\n",
    "set_seed(seed0)\n",
    "# split & loaders\n",
    "train_ds, val_ds = PKLDataset.split_dataset(train_path_1)\n",
    "loader_tr = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "loader_va = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "# instantiate, train, evaluate\n",
    "source_model = CNN().to(device)\n",
    "opt0   = optim.Adam(source_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "sch0   = optim.lr_scheduler.StepLR(opt0, step_size=50, gamma=0.1)\n",
    "source_model = train_model(source_model, loader_tr, criterion, opt0, sch0, num_epochs=10, device=device)\n",
    "print(f\"Source (phase1) val-acc: {eval_model(source_model, loader_va, device):.2f}%\")\n",
    "\n",
    "# save once\n",
    "pretrained_state = source_model.state_dict()\n",
    "\n",
    "\n",
    "results = {t:{vp:[] for vp in val_paths} for t in transfer_sets}\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n>>> Adaptation seed {seed}\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    for t in transfer_sets:\n",
    "        # reload student from the single pretrained source\n",
    "        student = CNN().to(device)\n",
    "        student.load_state_dict(pretrained_state)\n",
    "        student.train()\n",
    "\n",
    "        # Freeze head initially\n",
    "        for p in student.classifier_head.parameters():\n",
    "            p.requires_grad = False\n",
    "        feat_params = (\n",
    "            list(student.conv1.parameters()) +\n",
    "            list(student.conv2.parameters()) +\n",
    "            list(student.conv3.parameters()) +\n",
    "            list(student.feature_extractor.parameters())\n",
    "        )\n",
    "\n",
    "        shot_optimizer = optim.Adam(feat_params, lr=1e-4, weight_decay=1e-5)\n",
    "        # smooth cosine decay over ALL epochs\n",
    "        total_epochs   = 100\n",
    "        shot_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            shot_optimizer, T_max=total_epochs, eta_min=1e-6\n",
    "        )\n",
    "\n",
    "        lambda_shot    = 1.0\n",
    "        im_only        = 50\n",
    "        pseudo_labels_all = None\n",
    "        pseudo_confidence = None\n",
    "\n",
    "        dataset_t      = PKLDataset(t)\n",
    "        unlab_loader   = DataLoader(dataset_t, batch_size=64, shuffle=True)\n",
    "        full_loader_t  = DataLoader(dataset_t, batch_size=64, shuffle=False)\n",
    "\n",
    "        for epoch in range(total_epochs):\n",
    "            student.train()\n",
    "\n",
    "            # --- Information Maximization ---\n",
    "            if epoch < im_only:\n",
    "                running_Hc = running_Hm = 0.0\n",
    "                for x, _ in unlab_loader:\n",
    "                    x = x.to(device)\n",
    "                    logits = student(x)\n",
    "                    probs  = torch.softmax(logits, dim=1)\n",
    "                    logp   = torch.log(probs + 1e-12)\n",
    "                    Hc     = -(probs * logp).sum(1).mean()\n",
    "                    p_bar  = probs.mean(0)\n",
    "                    Hm     = -(p_bar * torch.log(p_bar + 1e-12)).sum()\n",
    "                    loss = Hc - lambda_shot * Hm\n",
    "\n",
    "                    shot_optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(feat_params, 1.0)\n",
    "                    shot_optimizer.step()\n",
    "\n",
    "                    running_Hc += Hc.item() * x.size(0)\n",
    "                    running_Hm += Hm.item() * x.size(0)\n",
    "\n",
    "                if (epoch+1)%20==0 or epoch==0:\n",
    "                    N = len(unlab_loader.dataset)\n",
    "                    print(f\"  [IM] Epoch {epoch+1}/{total_epochs}  \"\n",
    "                          f\"H_cond={(running_Hc/N):.4f}  H_marg={(running_Hm/N):.4f}\")\n",
    "\n",
    "                shot_scheduler.step()\n",
    "\n",
    "                # compute pseudo‐labels at end of IM\n",
    "                if epoch == im_only - 1:\n",
    "                    student.eval()\n",
    "                    feats, probs_all = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for x, _ in full_loader_t:\n",
    "                            x = x.to(device)\n",
    "                            f = student.extract_features(x)\n",
    "                            p = torch.softmax(student.classifier_head(f), dim=1)\n",
    "                            feats.append(f.cpu())\n",
    "                            probs_all.append(p.cpu())\n",
    "                    feats = torch.cat(feats)\n",
    "                    probs_all = torch.cat(probs_all)\n",
    "\n",
    "                    # centroids & hard assignments\n",
    "                    C = probs_all.size(1)\n",
    "                    centroids = torch.zeros(C, feats.size(1))\n",
    "                    for k in range(C):\n",
    "                        w = probs_all[:,k].unsqueeze(1)\n",
    "                        denom = w.sum()\n",
    "                        if denom>0:\n",
    "                            centroids[k] = (w*feats).sum(0)/denom\n",
    "                    dists = torch.cdist(feats, centroids)\n",
    "                    pseudo_labels_all  = torch.argmin(dists, dim=1)\n",
    "                    pseudo_confidence = probs_all.max(1).values\n",
    "\n",
    "                    # unfreeze head & add it to optimizer with lower LR\n",
    "                    for p in student.classifier_head.parameters():\n",
    "                        p.requires_grad = True\n",
    "                    shot_optimizer.add_param_group({\n",
    "                        'params': student.classifier_head.parameters(),\n",
    "                        'lr': shot_optimizer.param_groups[0]['lr'] * 0.2,\n",
    "                        'weight_decay': 1e-5\n",
    "                    })\n",
    "                    student.train()\n",
    "\n",
    "            # --- Pseudo‐label Fine‐tuning ---\n",
    "            else:\n",
    "                if pseudo_labels_all is None:\n",
    "                    raise RuntimeError(\"No pseudo-labels!\")\n",
    "                running_pl = 0.0\n",
    "                # shuffle indices each epoch\n",
    "                idxs = torch.randperm(len(dataset_t)).tolist()\n",
    "                pl_subset = Subset(dataset_t, idxs)\n",
    "                pl_loader = DataLoader(pl_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "                for b_idx, (x, _) in enumerate(pl_loader):\n",
    "                    start = b_idx * pl_loader.batch_size\n",
    "                    end   = start + x.size(0)\n",
    "                    batch_idx = idxs[start:end]\n",
    "                    conf = pseudo_confidence[batch_idx].to(device)\n",
    "                    labels = pseudo_labels_all[batch_idx].to(device)\n",
    "\n",
    "                    x = x.to(device)\n",
    "                    f = student.extract_features(x)\n",
    "                    logits = student.classifier_head(f)\n",
    "\n",
    "                    # confidence‐weighted loss\n",
    "                    per_sample = nn.CrossEntropyLoss(reduction='none')(logits, labels)\n",
    "                    # optional thresholding to skip very low‐conf samples\n",
    "                    mask = conf >= 0.3\n",
    "                    if mask.sum()>0:\n",
    "                        loss = (per_sample[mask] * conf[mask]).mean()\n",
    "                    else:\n",
    "                        loss = per_sample.mean()\n",
    "\n",
    "                    shot_optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(shot_optimizer.param_groups[0]['params'], 1.0)\n",
    "                    shot_optimizer.step()\n",
    "\n",
    "                    running_pl += loss.item() * x.size(0)\n",
    "\n",
    "                if (epoch+1)%20==0 or epoch==im_only:\n",
    "                    N = len(dataset_t)\n",
    "                    print(f\"  [PL] Epoch {epoch+1}/{total_epochs}  Pseudo‐CE Loss: {(running_pl/N):.4f}\")\n",
    "\n",
    "                shot_scheduler.step()\n",
    "\n",
    "        # evaluate on each target val set\n",
    "        student.eval()\n",
    "        for vp in val_paths:\n",
    "            vl = DataLoader(PKLDataset(vp), batch_size=64, shuffle=False)\n",
    "            acc = eval_model(student, vl, device)\n",
    "            results[t][vp].append(acc)\n",
    "            print(f\"  → {vp}: {acc:.2f}%\")\n",
    "\n",
    "# final summary\n",
    "print(\"\\n=== Mean ± Std Dev over seeds ===\")\n",
    "for t in transfer_sets:\n",
    "    for vp in val_paths:\n",
    "        arr = np.array(results[t][vp])\n",
    "        print(f\"{t} → {vp}: mean={arr.mean():.2f}%, std={arr.std(ddof=1):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiwi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
