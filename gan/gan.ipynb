{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from pkldataset import PKLDataset\n",
    "import random\n",
    "import numpy as np\n",
    "import form, gen\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_model(input_length: int = 2800, num_classes: int = 10, input_channels: int = 1):\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv1d(input_channels, 16, kernel_size=31, padding=15),\n",
    "                nn.BatchNorm1d(16),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool1d(2)\n",
    "            )\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv1d(16, 32, kernel_size=31, padding=15),\n",
    "                nn.BatchNorm1d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool1d(2)\n",
    "            )\n",
    "            self.conv3 = nn.Sequential(\n",
    "                nn.Conv1d(32, 64, kernel_size=31, padding=15),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool1d(2)\n",
    "            )\n",
    "            conv_output_length = input_length // 8\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(64 * conv_output_length, 128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(128, num_classes)\n",
    "            )\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            if x.dim() == 2:\n",
    "                x = x.unsqueeze(1)\n",
    "            x = self.conv1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.conv3(x)\n",
    "            return self.fc(x)\n",
    "    return CNN()\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training dataset names\n",
    "train_paths = [\"../datasets/RPDC197/train_20\", \"../datasets/RPDC197/train_50\", \"../datasets/RPDC197/train_100\", \"../datasets/RPDC197/train_200\", \"../datasets/RPDC197/train_300\",\n",
    " \"../datasets/RPDC197/train_400\", \"../datasets/RPDC197/train_500\", \"../datasets/RPDC197/train_600\"]\n",
    "\n",
    "# Validation datasets to test each model on\n",
    "val_paths = [\n",
    "    \"../datasets/RPDC185/val_1000\",\n",
    "    \"../datasets/RPDC188/val_1000\",\n",
    "    \"../datasets/RPDC191/val_1000\",\n",
    "    \"../datasets/RPDC194/val_1000\",\n",
    "    \"../datasets/RPDC197/val_1000\",\n",
    "]\n",
    "\n",
    "seeds = [101, 202, 303, 404, 505, 606, 707, 808, 909, 1001]\n",
    "\n",
    "# 2) prepare a nested results dict: { train_path: { val_path: [acc, acc, ...] } }\n",
    "results = { tp: { vp: [] for vp in val_paths } for tp in train_paths }\n",
    "\n",
    "# === Generator Pretraining ===\n",
    "train_dataset_1 = PKLDataset(r\"C:\\Users\\gus07\\Desktop\\data hiwi\\preprocessing\\HC\\T197\\RP\")\n",
    "train_loader_1 = DataLoader(train_dataset_1, batch_size=64, shuffle=True)\n",
    "\n",
    "gen.generate(train_loader_1, num_epochs=150, num_samples=10,\n",
    "             save_new_generator_path=\"generator_model.pth\")\n",
    "for seed in seeds:\n",
    "    print(f\"\\n=== Seed {seed} ===\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    for train_path in train_paths:\n",
    "        print(f\"--- Transfer Learning on {train_path} ---\")\n",
    "        # load real\n",
    "        ds_real = PKLDataset(train_path)\n",
    "        loader_real = DataLoader(ds_real, batch_size=64, shuffle=True)\n",
    "\n",
    "        # generate synthetic under the same seed\n",
    "        gen.generate(\n",
    "            loader_real,\n",
    "            num_epochs=150,\n",
    "            num_samples=20,\n",
    "            pretrained_generator_path=\"generator_model.pth\"\n",
    "        )\n",
    "        form.format()\n",
    "\n",
    "        # build combined dataset\n",
    "        synth_ds = PKLDataset(\"synth_data/individual_samples\")\n",
    "        combined = ConcatDataset([ds_real, synth_ds])\n",
    "        loader_comb = DataLoader(combined, batch_size=32, shuffle=True)\n",
    "\n",
    "        # build & train model\n",
    "        model = get_model().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "        def train_model(model, train_loader, criterion, optimizer, scheduler,\n",
    "                        num_epochs=100, device=device, max_grad_norm=1.0):\n",
    "            best_loss = float('inf')\n",
    "            best_state = None\n",
    "            for ep in range(num_epochs):\n",
    "                model.train()\n",
    "                running = 0.0\n",
    "                for X, Y in train_loader:\n",
    "                    X, Y = X.to(device), Y.to(device)\n",
    "                    y_idx = Y.argmax(dim=1)\n",
    "                    optimizer.zero_grad()\n",
    "                    out = model(X)\n",
    "                    loss = criterion(out, y_idx)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                    optimizer.step()\n",
    "                    running += loss.item() * X.size(0)\n",
    "                scheduler.step()\n",
    "                epoch_loss = running / len(train_loader.dataset)\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_state = model.state_dict()\n",
    "            if best_state:\n",
    "                model.load_state_dict(best_state)\n",
    "            return model\n",
    "\n",
    "        model = train_model(model, loader_comb, criterion, optimizer, scheduler)\n",
    "\n",
    "        # 5) evaluate & append to results\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for vp in val_paths:\n",
    "                val_ds = PKLDataset(vp)\n",
    "                val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "                correct = total = 0\n",
    "                for X, Y in val_loader:\n",
    "                    X, Y = X.to(device), Y.to(device)\n",
    "                    y_idx = Y.argmax(dim=1)\n",
    "                    preds = model(X).argmax(dim=1)\n",
    "                    correct += (preds == y_idx).sum().item()\n",
    "                    total += Y.size(0)\n",
    "                acc = 100. * correct / total\n",
    "                results[train_path][vp].append(acc)\n",
    "                print(f\"[{train_path} â†’ {vp}] Seed {seed}: {acc:.2f}%\")\n",
    "\n",
    "# 6) after all seeds, compute mean & std\n",
    "print(\"\\n=== Summary across seeds ===\")\n",
    "for tp in train_paths:\n",
    "    for vp in val_paths:\n",
    "        arr  = np.array(results[tp][vp])\n",
    "        mean = arr.mean()\n",
    "        std  = arr.std(ddof=1)\n",
    "        print(f\"{tp} -> {vp}: Mean = {mean:.2f}%, Std = {std:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiwi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
