{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from pkldataset import PKLDataset\n",
    "import random\n",
    "import numpy as np\n",
    "import form, gen\n",
    "from helpers import set_seed, get_model, eval_model, train_model\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training dataset names\n",
    "train_paths = [\"../datasets/RPDC197/train_20\", \"../datasets/RPDC197/train_50\", \"../datasets/RPDC197/train_100\", \"../datasets/RPDC197/train_200\", \"../datasets/RPDC197/train_300\",\n",
    " \"../datasets/RPDC197/train_400\", \"../datasets/RPDC197/train_500\", \"../datasets/RPDC197/train_600\"]\n",
    "\n",
    "# Validation datasets to test each model on\n",
    "val_paths = [\n",
    "    \"../datasets/RPDC185/val_1000\",\n",
    "    \"../datasets/RPDC188/val_1000\",\n",
    "    \"../datasets/RPDC191/val_1000\",\n",
    "    \"../datasets/RPDC194/val_1000\",\n",
    "    \"../datasets/RPDC197/val_1000\",\n",
    "]\n",
    "\n",
    "seeds = [101, 202, 303, 404, 505, 606, 707, 808, 909, 1001]\n",
    "\n",
    "# 2) prepare a nested results dict: { train_path: { val_path: [acc, acc, ...] } }\n",
    "results = { tp: { vp: [] for vp in val_paths } for tp in train_paths }\n",
    "\n",
    "# === Generator Pretraining ===\n",
    "train_dataset_1 = PKLDataset(r\"C:\\Users\\gus07\\Desktop\\data hiwi\\preprocessing\\HC\\T197\\RP\")\n",
    "train_loader_1 = DataLoader(train_dataset_1, batch_size=64, shuffle=True)\n",
    "\n",
    "gen.generate(train_loader_1, num_epochs=150, num_samples=10,\n",
    "             save_new_generator_path=\"generator_model.pth\")\n",
    "for seed in seeds:\n",
    "    print(f\"\\n=== Seed {seed} ===\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    for train_path in train_paths:\n",
    "        print(f\"--- Transfer Learning on {train_path} ---\")\n",
    "        # load real\n",
    "        ds_real = PKLDataset(train_path)\n",
    "        loader_real = DataLoader(ds_real, batch_size=64, shuffle=True)\n",
    "\n",
    "        # generate synthetic under the same seed\n",
    "        gen.generate(\n",
    "            loader_real,\n",
    "            num_epochs=150,\n",
    "            num_samples=20,\n",
    "            pretrained_generator_path=\"generator_model.pth\"\n",
    "        )\n",
    "        form.format()\n",
    "\n",
    "        # build combined dataset\n",
    "        synth_ds = PKLDataset(\"synth_data/individual_samples\")\n",
    "        combined = ConcatDataset([ds_real, synth_ds])\n",
    "        loader_comb = DataLoader(combined, batch_size=32, shuffle=True)\n",
    "\n",
    "        # build & train model\n",
    "        model = get_model().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "        model = train_model(model, loader_comb, criterion,\n",
    "                                optimizer, scheduler, num_epochs=50,\n",
    "                                device=device)\n",
    "\n",
    "        # 5) evaluate & append to results\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for vp in val_paths:\n",
    "                val_ds = PKLDataset(vp)\n",
    "                val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "                correct = total = 0\n",
    "                for X, Y in val_loader:\n",
    "                    X, Y = X.to(device), Y.to(device)\n",
    "                    y_idx = Y.argmax(dim=1)\n",
    "                    preds = model(X).argmax(dim=1)\n",
    "                    correct += (preds == y_idx).sum().item()\n",
    "                    total += Y.size(0)\n",
    "                acc = 100. * correct / total\n",
    "                results[train_path][vp].append(acc)\n",
    "                print(f\"[{train_path} â†’ {vp}] Seed {seed}: {acc:.2f}%\")\n",
    "\n",
    "# 6) after all seeds, compute mean & std\n",
    "print(\"\\n=== Summary across seeds ===\")\n",
    "for tp in train_paths:\n",
    "    for vp in val_paths:\n",
    "        arr  = np.array(results[tp][vp])\n",
    "        mean = arr.mean()\n",
    "        std  = arr.std(ddof=1)\n",
    "        print(f\"{tp} -> {vp}: Mean = {mean:.2f}%, Std = {std:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiwi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
