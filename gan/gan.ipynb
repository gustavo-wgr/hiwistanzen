{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generator-Augmented Transfer Learning Performance\n",
        "\n",
        "This notebook pre-trains a data generator on a source dataset, then for each training subset:\n",
        "1. Generates synthetic samples\n",
        "2. Combines them with real data\n",
        "3. Trains a classifier and evaluates on multiple validation sets across multiple seeds\n",
        "Results are summarized as mean ± std for each (train → val) pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Setup\n",
        "\n",
        "Load libraries, modules for data generation and formatting, define device and loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, random\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "\n",
        "from pkldataset import PKLDataset\n",
        "import form, gen\n",
        "from helpers import set_seed, get_model, train_model, eval_model\n",
        "\n",
        "# Device and loss\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Paths for subsets and validation\n",
        "train_paths = [\n",
        "    \"../datasets/RPDC197/train_20\",\n",
        "    \"../datasets/RPDC197/train_50\",\n",
        "    \"../datasets/RPDC197/train_100\",\n",
        "    \"../datasets/RPDC197/train_200\",\n",
        "    \"../datasets/RPDC197/train_300\",\n",
        "    \"../datasets/RPDC197/train_400\",\n",
        "    \"../datasets/RPDC197/train_500\",\n",
        "    \"../datasets/RPDC197/train_600\",\n",
        "]\n",
        "\n",
        "val_paths = [\n",
        "    \"../datasets/RPDC185/val_1000\",\n",
        "    \"../datasets/RPDC188/val_1000\",\n",
        "    \"../datasets/RPDC191/val_1000\",\n",
        "    \"../datasets/RPDC194/val_1000\",\n",
        "    \"../datasets/RPDC197/val_1000\",\n",
        "]\n",
        "\n",
        "seeds = [101, 202, 303, 404, 505, 606, 707, 808, 909, 1001]\n",
        "\n",
        "# Results container\n",
        "results = {tp: {vp: [] for vp in val_paths} for tp in train_paths}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f677f281",
      "metadata": {},
      "source": [
        "## 2. Generator Pretraining\n",
        "\n",
        "Train the generative model once on a primary dataset before transfer experiments.\n",
        "The 10 synthetic samples generated here are not used. The goal is only to pretrain the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08baaa9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pretrain generator on a source dataset\n",
        "train_dataset_1 = PKLDataset(r\"C:\\Users\\gus07\\Desktop\\data hiwi\\preprocessing\\HC\\T197\\RP\")\n",
        "train_loader_1 = DataLoader(train_dataset_1, batch_size=64, shuffle=True)\n",
        "\n",
        "gen.generate(\n",
        "    train_loader_1,\n",
        "    num_epochs=150,\n",
        "    num_samples=10,\n",
        "    save_new_generator_path=\"generator_model.pth\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "011f8c7c",
      "metadata": {},
      "source": [
        "## 3. Seeded Generator-Augmented Training & Evaluation\n",
        "The synthetic generation ensures class balance by generating an equal number of samples per\n",
        "class. The labels are randomly permuted before generation to avoid ordering bias. Generated\n",
        "inputs and labels are saved as a pickle file (`generated data.pkl`) which is later processed\n",
        "by `form.py` to separate the generated data into individual samples.\n",
        "\n",
        "For each seed and each training subset:\n",
        "1. Generate synthetic data\n",
        "2. Format synthetic samples\n",
        "3. Combine real + synthetic data\n",
        "4. Train classifier for 50 epochs\n",
        "5. Evaluate on validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for seed in seeds:\n",
        "    print(f\"\\n=== Seed {seed} ===\")\n",
        "    set_seed(seed)\n",
        "    \n",
        "    for train_path in train_paths:\n",
        "        print(f\"--- Transfer Learning on {train_path} ---\")\n",
        "        # Load real data\n",
        "        ds_real = PKLDataset(train_path)\n",
        "        loader_real = DataLoader(ds_real, batch_size=64, shuffle=True)\n",
        "\n",
        "        # Generate synthetic under same seed\n",
        "        gen.generate(\n",
        "            loader_real,\n",
        "            num_epochs=150,\n",
        "            num_samples=20,\n",
        "            pretrained_generator_path=\"generator_model.pth\"\n",
        "        )\n",
        "        form.format()\n",
        "\n",
        "        # Build combined dataset\n",
        "        synth_ds = PKLDataset(\"synth_data/individual_samples\")\n",
        "        combined = ConcatDataset([ds_real, synth_ds])\n",
        "        loader_comb = DataLoader(combined, batch_size=32, shuffle=True)\n",
        "\n",
        "        # Train classifier\n",
        "        model = get_model().to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "\n",
        "        model = train_model(\n",
        "            model,\n",
        "            loader_comb,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            scheduler,\n",
        "            num_epochs=50,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for vp in val_paths:\n",
        "                val_loader = DataLoader(PKLDataset(vp), batch_size=64, shuffle=False)\n",
        "                correct = total = 0\n",
        "                for X, Y in val_loader:\n",
        "                    X, Y = X.to(device), Y.to(device)\n",
        "                    y_idx = Y.argmax(dim=1)\n",
        "                    preds = model(X).argmax(dim=1)\n",
        "                    correct += (preds == y_idx).sum().item()\n",
        "                    total += Y.size(0)\n",
        "                acc = 100. * correct / total\n",
        "                results[train_path][vp].append(acc)\n",
        "                print(f\"[{train_path} → {vp}] Seed {seed}: {acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Summary of Results\n",
        "\n",
        "Compute mean and standard deviation of accuracy across seeds for each (train → validation) pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Summary across seeds ===\")\n",
        "for tp in train_paths:\n",
        "    for vp in val_paths:\n",
        "        arr = np.array(results[tp][vp])\n",
        "        mean, std = arr.mean(), arr.std(ddof=1)\n",
        "        print(f\"{tp} -> {vp}: Mean = {mean:.2f}%, Std = {std:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hiwi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
