{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d1d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import random\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from pkldataset import PKLDataset, NoisyPKLDataset\n",
    "import gen\n",
    "import form\n",
    "\n",
    "# Utility to set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Model definition\n",
    "def get_model(input_length: int = 2800, num_classes: int = 10, input_channels: int = 1):\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv1d(input_channels, 16, kernel_size=31, padding=15),\n",
    "                nn.BatchNorm1d(16),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool1d(kernel_size=2)\n",
    "            )\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.Conv1d(16, 32, kernel_size=31, padding=15),\n",
    "                nn.BatchNorm1d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool1d(kernel_size=2)\n",
    "            )\n",
    "            self.conv3 = nn.Sequential(\n",
    "                nn.Conv1d(32, 64, kernel_size=31, padding=15),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool1d(kernel_size=2)\n",
    "            )\n",
    "            conv_output_length = input_length // 8\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(64 * conv_output_length, 128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(128, num_classes)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            if x.dim() == 2:\n",
    "                x = x.unsqueeze(1)\n",
    "            x = self.conv1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.conv3(x)\n",
    "            return self.fc_layers(x)\n",
    "    return CNN()\n",
    "\n",
    "# Training with validation for Phase 1\n",
    "\n",
    "def train_model_phase1(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "                       num_epochs=10, device=torch.device(\"cpu\"), max_grad_norm=1.0):\n",
    "    best_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            targets = y.argmax(dim=1)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "        scheduler.step()\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                targets = y.argmax(dim=1)\n",
    "                out = model(x)\n",
    "                pred = out.argmax(dim=1)\n",
    "                correct += (pred == targets).sum().item()\n",
    "                total += y.size(0)\n",
    "        acc = 100. * correct / total\n",
    "        print(f\"Phase1 Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Val Acc: {acc:.2f}%\")\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_state = model.state_dict()\n",
    "    # load best state\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "# Evaluation on arbitrary loader\n",
    "def eval_model(model, loader, device=torch.device(\"cpu\")):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            targets = y.argmax(dim=1)\n",
    "            out = model(x)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == targets).sum().item()\n",
    "            total += y.size(0)\n",
    "    return 100. * correct / total\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration\n",
    "    train_path_1 = r\"C:\\Users\\gus07\\Desktop\\data hiwi\\preprocessing\\HC\\T197\\RP\"\n",
    "    pretrained_model_path = \"cnn_model.pth\"\n",
    "    train_sizes = [\"../datasets/RPDC197/train_20\", \"../datasets/RPDC197/train_50\", \"../datasets/RPDC197/train_100\", \"../datasets/RPDC197/train_200\", \"../datasets/RPDC197/train_300\",\n",
    " \"../datasets/RPDC197/train_400\", \"../datasets/RPDC197/train_500\", \"../datasets/RPDC197/train_600\"]\n",
    "\n",
    "    # Validation datasets to test each model on\n",
    "    val_paths = [\n",
    "        \"../datasets/RPDC185/val_1000\",\n",
    "        \"../datasets/RPDC188/val_1000\",\n",
    "        \"../datasets/RPDC191/val_1000\",\n",
    "        \"../datasets/RPDC194/val_1000\",\n",
    "        \"../datasets/RPDC197/val_1000\",\n",
    "    ]\n",
    "    seeds = [101,202,303,404,505,606,707,808,909,1001]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Phase 1: Pretrain on original split\n",
    "    ds_train1, ds_val1 = PKLDataset.split_dataset(train_path_1)\n",
    "    loader_train1 = DataLoader(ds_train1, batch_size=64, shuffle=True)\n",
    "    loader_val1 = DataLoader(ds_val1, batch_size=64, shuffle=True)\n",
    "    model_phase1 = get_model().to(device)\n",
    "    opt1 = optim.Adam(model_phase1.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    sch1 = optim.lr_scheduler.StepLR(opt1, step_size=50, gamma=0.1)\n",
    "    print(\"=== Phase 1: Pretraining ===\")\n",
    "    model_phase1 = train_model_phase1(model_phase1, loader_train1, loader_val1,\n",
    "                                      criterion, opt1, sch1,\n",
    "                                      num_epochs=10, device=device)\n",
    "    torch.save(model_phase1.state_dict(), pretrained_model_path)\n",
    "    gen.generate(loader_train1, num_epochs=150, num_samples=10,\n",
    "             save_new_generator_path=\"generator_model.pth\",)\n",
    "\n",
    "    # Container for results\n",
    "    results = {t: {vp: [] for vp in val_paths} for t in train_sizes}\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n>>> Seed {seed}\")\n",
    "        set_seed(seed)\n",
    "        # Phase 2: Transfer + eval\n",
    "        for t in train_sizes:\n",
    "            print(f\"-- Transfer on {t}\")\n",
    "            # Synthetic generation\n",
    "            ds_t = PKLDataset(t)\n",
    "            loader_t = DataLoader(ds_t, batch_size=64, shuffle=True)\n",
    "            gen.generate(loader_t,\n",
    "                         num_epochs=150,\n",
    "                         num_samples=20,\n",
    "                         pretrained_generator_path=\"generator_model.pth\")\n",
    "            form.format()\n",
    "            # Load synthetic and noisy sets\n",
    "            ds_synth = PKLDataset(\"synth_data/individual_samples\")\n",
    "            ds_noisy = NoisyPKLDataset(t)\n",
    "            combined = ConcatDataset([ds_t, ds_synth, ds_noisy])\n",
    "            loader_comb = DataLoader(combined, batch_size=32, shuffle=True)\n",
    "\n",
    "            # Initialize and load pretrained\n",
    "            net = get_model().to(device)\n",
    "            net.load_state_dict(torch.load(pretrained_model_path))\n",
    "            opt2 = optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "            sch2 = optim.lr_scheduler.StepLR(opt2, step_size=50, gamma=0.1)\n",
    "\n",
    "            # Train on combined\n",
    "            net = train_model_phase1(net, loader_comb, loader_val1,\n",
    "                                     criterion, opt2, sch2,\n",
    "                                     num_epochs=100, device=device)\n",
    "\n",
    "            # Evaluate on all validation splits\n",
    "            for vp in val_paths:\n",
    "                ds_vp = PKLDataset(vp)\n",
    "                loader_vp = DataLoader(ds_vp, batch_size=64, shuffle=False)\n",
    "                acc = eval_model(net, loader_vp, device)\n",
    "                results[t][vp].append(acc)\n",
    "                print(f\"Seed {seed}, {t} -> {vp}: {acc:.2f}%\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n=== Summary over seeds ===\")\n",
    "    for t in train_sizes:\n",
    "        for vp in val_paths:\n",
    "            arr = np.array(results[t][vp])\n",
    "            mean = arr.mean()\n",
    "            std = arr.std(ddof=1)\n",
    "            print(f\"{t} -> {vp}: Mean={mean:.2f}%, Std={std:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiwi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
