{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hybrid Generative & Noisy-Augmented Transfer Learning Pipeline\n",
        "\n",
        "This notebook:\n",
        "1. Pretrains a CNN on a primary dataset split with supervised training (Phase 1).\n",
        "2. Trains a data generator and saves its weights.\n",
        "3. For each seed and training subset:\n",
        "   - Generates synthetic samples\n",
        "   - Formats synthetic data\n",
        "   - Combines real, synthetic, and noisy data\n",
        "   - Fine-tunes the pretrained CNN (Phase 2)\n",
        "   - Evaluates on multiple validation sets\n",
        "Results are summarized as mean ± std for each train → validation pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports, Utilities & Model Definition\n",
        "\n",
        "Load libraries, define reproducibility and model, and helper training/evaluation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, random\n",
        "sys.path.insert(0, os.path.abspath('..'))\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from pkldataset import PKLDataset, NoisyPKLDataset\n",
        "import gen, form\n",
        "from helpers import set_seed, get_model, eval_model\n",
        "\n",
        "# Training with validation for Phase 1\n",
        "def train_model_phase1(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                       num_epochs=10, device=torch.device('cpu'), max_grad_norm=1.0):\n",
        "    best_loss = float('inf')\n",
        "    best_state = None\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            targets = y.argmax(dim=1)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, targets)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * x.size(0)\n",
        "        scheduler.step()\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        model.eval()\n",
        "        correct = total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                targets = y.argmax(dim=1)\n",
        "                pred = model(x).argmax(dim=1)\n",
        "                correct += (pred == targets).sum().item()\n",
        "                total += y.size(0)\n",
        "        acc = 100. * correct / total\n",
        "        print(f\"Phase1 Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Val Acc: {acc:.2f}%\")\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_state = model.state_dict()\n",
        "    model.load_state_dict(best_state)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "- **Primary dataset**: path to pretraining folder  \n",
        "- **Generator & Model checkpoints**  \n",
        "- **Training subsets**: for Phase 2  \n",
        "- **Validation sets**: for evaluation  \n",
        "- **Seeds**: for reproducibility  \n",
        "- **Results container**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths and settings\n",
        "train_path_1 = r\"C:\\Users\\gus07\\Desktop\\data hiwi\\preprocessing\\HC\\T197\\RP\"\n",
        "pretrained_model_path = \"cnn_model.pth\"\n",
        "\n",
        "train_sizes = [\n",
        "    \"../datasets/RPDC197/train_20\",\n",
        "    \"../datasets/RPDC197/train_50\",\n",
        "    \"../datasets/RPDC197/train_100\",\n",
        "    \"../datasets/RPDC197/train_200\",\n",
        "    \"../datasets/RPDC197/train_300\",\n",
        "    \"../datasets/RPDC197/train_400\",\n",
        "    \"../datasets/RPDC197/train_500\",\n",
        "    \"../datasets/RPDC197/train_600\",\n",
        "]\n",
        "val_paths = [\n",
        "    \"../datasets/RPDC185/val_1000\",\n",
        "    \"../datasets/RPDC188/val_1000\",\n",
        "    \"../datasets/RPDC191/val_1000\",\n",
        "    \"../datasets/RPDC194/val_1000\",\n",
        "    \"../datasets/RPDC197/val_1000\",\n",
        "]\n",
        "seeds = [101,202,303,404,505,606,707,808,909,1001]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "results = {t: {vp: [] for vp in val_paths} for t in train_sizes}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Phase 1: Supervised Pretraining & Generator Training\n",
        "\n",
        "Split the primary dataset, train CNN, save model, then train generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from pkldataset import PKLDataset\n",
        "\n",
        "ds_train1, ds_val1 = PKLDataset.split_dataset(train_path_1)\n",
        "loader_train1 = DataLoader(ds_train1, batch_size=64, shuffle=True)\n",
        "loader_val1 = DataLoader(ds_val1, batch_size=64, shuffle=True)\n",
        "\n",
        "print(\"=== Phase 1: Supervised Pretraining ===\")\n",
        "model_phase1 = get_model().to(device)\n",
        "opt1 = optim.Adam(model_phase1.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "sch1 = optim.lr_scheduler.StepLR(opt1, step_size=50, gamma=0.1)\n",
        "model_phase1 = train_model_phase1(\n",
        "    model_phase1, loader_train1, loader_val1, criterion, opt1, sch1,\n",
        "    num_epochs=10, device=device\n",
        ")\n",
        "torch.save(model_phase1.state_dict(), pretrained_model_path)\n",
        "\n",
        "print(\"=== Phase 1: Generator Training ===\")\n",
        "gen.generate(loader_train1, num_epochs=150, num_samples=10, save_new_generator_path=\"generator_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Phase 2: Transfer with Synthetic & Noisy Augmentation\n",
        "\n",
        "For each seed and training subset:\n",
        "1. Generate synthetic samples\n",
        "2. Format synthetic data\n",
        "3. Combine real, synthetic, and noisy datasets\n",
        "4. Fine-tune pretrained CNN with supervised training\n",
        "5. Evaluate on validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for seed in seeds:\n",
        "    print(f\"\\n>>> Seed {seed}\")\n",
        "    set_seed(seed)\n",
        "    for t in train_sizes:\n",
        "        print(f\"-- Transfer on {t}\")\n",
        "        ds_t = PKLDataset(t)\n",
        "        loader_t = DataLoader(ds_t, batch_size=64, shuffle=True)\n",
        "        gen.generate(loader_t, num_epochs=150, num_samples=20, pretrained_generator_path=\"generator_model.pth\")\n",
        "        form.format()\n",
        "        ds_synth = PKLDataset(\"synth_data/individual_samples\")\n",
        "        ds_noisy = NoisyPKLDataset(t)\n",
        "        combined = ConcatDataset([ds_t, ds_synth, ds_noisy])\n",
        "        loader_comb = DataLoader(combined, batch_size=32, shuffle=True)\n",
        "        net = get_model().to(device)\n",
        "        net.load_state_dict(torch.load(pretrained_model_path))\n",
        "        opt2 = optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "        sch2 = optim.lr_scheduler.StepLR(opt2, step_size=50, gamma=0.1)\n",
        "        net = train_model_phase1(net, loader_comb, loader_val1, criterion, opt2, sch2, num_epochs=100, device=device)\n",
        "        for vp in val_paths:\n",
        "            ds_vp = PKLDataset(vp)\n",
        "            loader_vp = DataLoader(ds_vp, batch_size=64, shuffle=False)\n",
        "            acc = eval_model(net, loader_vp, device)\n",
        "            results[t][vp].append(acc)\n",
        "            print(f\"Seed {seed}, {t} -> {vp}: {acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary of Results\n",
        "\n",
        "Compute mean and std deviation over seeds for each train → validation pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Summary over seeds ===\")\n",
        "for t in train_sizes:\n",
        "    for vp in val_paths:\n",
        "        arr = np.array(results[t][vp])\n",
        "        print(f\"{t} -> {vp}: Mean={arr.mean():.2f}%, Std={arr.std(ddof=1):.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hiwi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
